\documentclass[a4paper,12pt,french]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{babel}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{longtable}
\usepackage{caption}
\usepackage{relsize}
\usepackage{xcolor}
\usepackage{enumitem}
\usepackage[document]{ragged2e}
\usepackage{hyperref}
\usepackage{etoolbox}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{epstopdf}
\usepackage[nottoc, notlof, notlot]{tocbibind}
\usepackage{fourier}
\usepackage{listings}

%opening
\title{DM RNDP}
\date{}
\author{Jules Berry, Thomas Poisson}

\theoremstyle{definition}
\newtheorem{definition}{D\'efinition}[section]
\AtEndEnvironment{definition}{\hfill $\square$}
\newtheorem{remarque}{Remarque}[section]

\theoremstyle{theorem}
\newtheorem{prop}{Proposition}[section]
\newtheorem{thm}{Th\'eor\`eme}[section]
\newtheorem{lemme}{Lemme}[section]
\newtheorem{corollaire}{Corollaire}[section]


\renewcommand\qedsymbol{$\blacksquare$}
\newcommand{\T}{\mathbb{T}}
\newcommand{\norm}[2]{\lVert{#1}\rVert_{#2}}
\newcommand{\sgn}{\text{sgn}}
\newcommand{\Lp}[1]{L^{#1}(\T)}
\newcommand{\C}{\mathcal{C}}
\newcommand{\module}[1]{\left \lvert #1 \right \rvert}
\newcommand{\scalaire}[2]{\left \langle {#1},\ {#2} \right \rangle}
\newcommand{\N}{\mathbb{N}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\scalaireo}[2]{\left \langle {#1},\ {#2} \right \rangle_{\omega}}



\begin{document}

\maketitle

\justify

\tableofcontents

\newpage

\section{Préliminaires}
\subsection{Question 1}
 \item On cherche une fonction $u \in \C^2([-1,1])$ telle que
 \begin{equation}\label{eq:problème_1}
  \begin{cases}
   -(\alpha(x) u'(x))' + \gamma(x)u(x) = f(x) \quad \forall x \in ] -1, 1[, \\
   u(-1) = c_1,\ u(1) = c_2
  \end{cases}.
 \end{equation}

 Soit $u$ un solution de ce problème et on pose
 \[
  v(x) = u(x) - c_1 \frac{x - 1}{2} - c_2 \frac{x+1}{2}
 \]
cette fonction est alors aussi $\C^2([-1,1])$ et on a de plus
\[
 v'(x) = u'(x) - \frac{c_1 + c_2}{2}, \quad v''(x) = u''(x).
\]
En injectant cette fonction dans l'EDO de \eqref{eq:problème_1} on a
\begin{align*}
 -(\alpha(x)v'(x))' + \gamma(x)v(x) &= -\alpha'(x)v'(x) - \alpha(x)v''(x) + \gamma(x)v(x) \\
 &= -(\alpha(x)u'(x))' + \frac{c_1 + c_2}{2}\alpha'(x) + \gamma(x)u(x) - \gamma(x)\left (c_1 \frac{x-1}{2} + c_2 \frac{x+1}{2} \right)
\end{align*}
On a donc que la fonction $v$ est solution du problème
\begin{equation}\label{eq:problème_2}
 \begin{cases}
  -(\alpha(x)v'(x))' + \gamma(x)v(x) = \tilde f(x) \\
  v(-1) = v(1) = 0
 \end{cases}
\end{equation}
où $\tilde f(x) = f(x) + \gamma(x)\left(c_1 \frac{x-1}{2} + c_2 \frac{x+1}{2}\right) - \frac{c_1 + c_2}{2}\alpha'(x)$.

De plus il est clair que $v(-1) = v(1) = 0$ et comme $f \in L^2(]-1,1[)$ , $\alpha \in \C^1([-1,1])$ et $\gamma \in L^\infty ([-1,1]) \subset L^2(]-1,1[)$ on a aussi $\tilde f \in L^2([-1,1])$.

On voit alors que le problème \eqref{eq:problème_2} est de la forme souhaitée. De plus si $v$ est une solution de ce problème et en posant $w(x) = v(x) + c_1 \frac{x - 1}{2} + c_2 \frac{x+1}{2}$, on peut voir que $w$ est une solution de \eqref{eq:problème_1}. On a donc montré que l'étude de ce dernier se ramène à l'étude de \eqref{eq:problème_2}.

\subsection{Question 2}

On pose $u = \rho v$ où $\rho : x \in [-1,1] \mapsto \exp \left ( \frac{1}{2} \int_{-1}^x \frac{\beta(t)}{\alpha(t)}\ dt \right)$.
Alors si $u$ est une solution de l'EDO
\begin{equation}
 -(\alpha u')' + \beta u' + \gamma u = f
\end{equation}
On remarque alors que
\begin{align*}
 \rho'(x) &= \frac{d}{dx} \exp \left ( \frac{1}{2} \int_{-1}^x \frac{\beta(t)}{\alpha(t)}\ dt \right) = \frac{\beta(x)}{2\alpha(x)} \exp \left ( \frac{1}{2} \int_{-1}^x \frac{\beta(t)}{\alpha(t)}\ dt \right) \\
 &= \frac{\beta(x)}{2\alpha(x)} \rho(x).
\end{align*}
Et par suite
\[
 \rho'' = \frac{1}{2} \left ( \frac{\beta' \alpha - \beta \alpha'}{\alpha^2}\rho + \frac{\beta}{\alpha} \rho'\right) = \frac{\beta' \alpha + 2 \beta^2 - \beta \alpha'}{2 \alpha^2}\rho.
\]

En réétudiant l'EDO on trouve donc
\begin{align*}
 -(\alpha u')' + \beta u' + \gamma u &= - \alpha' (\rho' v + \rho v') - \alpha ( \rho '' v + 2 \rho' v' + \rho v'') + \beta (\rho' v + \rho v') + \gamma \rho v \\
 &= \rho \left ( -\frac{\alpha' \beta v}{\alpha} - \alpha' v' - \frac{\beta' v}{2} - \frac{\beta^2 v}{\alpha} + \frac{\beta \alpha' v }{\alpha} - \beta v' - \alpha v'' + \frac{\beta^2 v}{\alpha} + \beta v' + \gamma v \right) = f
\end{align*}
Par définition de la fonction de $\rho$ celle-ci est strictement positive et on peut donc diviser par $\rho$ dans l'équation. Alors en simplifiant le membre de gauche on trouve
\[
 - \alpha' v' - \alpha v'' - \frac{1}{2} \beta' v +\gamma v = \frac{f}{\rho}.
\]
Alors en posant $\delta = \gamma - \frac{1}{2}\beta'$ et $g = \frac{f}{\rho}$. On trouve donc
\begin{equation}\label{eq:EDO_1}
 -(\alpha v')' + \delta v = g.
\end{equation}
On remarque au passage que nous avons supposé que la fonction $\beta$ était au moins $\C^1$.

\subsection{Question 3}

On considère l'EDO $-(\alpha u')' + \gamma u = f$ sur $[a,b]$. En posant
\[
 h : x \in [-1,1] \mapsto a \frac{x-1}{2} + b \frac{x+1}{2}
\]
et on considérant la fonction $v : x \in [-1,1] \mapsto u \circ h(x)$ on trouve que $v(-1) = u(a)$ et $v(1) = u(b)$. De plus en posant aussi $\tilde \alpha = \alpha \circ h$, $\tilde \gamma = \frac{b - a}{2} \gamma \circ h$ on a en supposant que $u$ est une solution de l'EDO
\begin{align*}
 -(\tilde \alpha(x) v'(x))' + \tilde \gamma(x) v(x) &= -(\alpha(h(x))(u(h(x)))')' + \frac{a + b}{2} \gamma(h(x))v(h(x)) \\
 &= -\frac{a + b}{2}( \alpha (h(x)) u'(h(x)))' + \frac{a + b}{2} \gamma(h(x)) u(h(x)) \\
 &= \frac{a+b}{2} f(h(x)).
\end{align*}
On peut alors poser $\tilde f(x) = \frac{a+b}{2} f \circ h (x)$ et on a alors que $v$ est solution de la nouvelle EDO
\[
 -(\tilde \alpha(x) v'(x))' + \tilde \gamma (x) u(x) = \tilde f(x).
\]
Comme la fonction $h$ est un polynôme de degrés un elle est bijective de $[-1,1]$ dans $[a,b]$. Alors si $v$ est solution de la dernière EDO on peut inverser la construction en posant $u =h^{-1} \circ v$ on voit que $u$ est solution de la première EDO.

\subsection{Quastion 4}

On cherche ici une solution $u \in H^2(]-1,1[)$ de

\begin{equation}\label{eq:problem_3}
 \begin{cases}
  -(\alpha(x) u'(x))' + \gamma(x) u(x) = f(x) \quad \forall x \in ]-1,1[ \\
  u(-1) = u(1) = 0
 \end{cases}.
\end{equation}


On commence par trouver une solution à la formulation variationnelle de \eqref{eq:problem_3} dans l'espace $H_0^1(]-1,1[)$. Soit $\phi \in H_0^1(]-1,1[)$ on a alors
\[
 -\int _{-1}^1 (\alpha(t)u'(t))' \phi(t)\ dt + \int _{-1}^1 \gamma(t) u(t) \phi(t) \ dt = \int _{-1}^1 f(t) \phi(t) \ dt.
\]
En intégrant par parties on trouve
\[
 \int _{-1}^1 (\alpha(t)u'(t)) \phi'(t)\ dt + \int _{-1}^1 \gamma(t) u(t) \phi(t) \ dt = \int _{-1}^1 f(t) \phi(t) \ dt.
\]

On cherche ensuite à appliquer le théorème de Lax-Milgram dans $H_0^1(]-1,1[)$. Pour cela on pose $a$ la forme bilinéaire
\[
 a : (\phi,\psi) \in (H_0^1(]-1,1[))^2 \mapsto \int _{-1}^1 (\alpha(t)\phi'(t)) \psi'(t)\ dt + \int _{-1}^1 \gamma(t) \phi(t) \psi(t) \ dt
\]
et $l$ la forme linéaire
\[
 l : \phi \in H_0^1(]-1,1[) \mapsto \int _{-1}^1 f(t) \phi(t) \ dt.
\]

On a alors
\[
 \module{\langle l, \phi \rangle} \leq  \int _{-1}^1 \module{f(t) \phi(t)} \ dt \leq \norm{f}{L^2}\norm{\phi}{L^2} \leq \norm{f}{L^2}\norm{\phi}{H^1}
\]
ce qui montre que $l$ est une forme linéaire continue sur $H_0^1(]-1,1[)$.
De plus on a aussi
\begin{align*}
 \module{a(\phi,\psi)} &\leq \int _{-1}^1 \module{\alpha(t)\phi'(t) \psi'(t)} \ dt + \int _{-1}^1 \module{\gamma(t) \phi(t) \psi(t)} \ dt \\
 & \leq \norm{\alpha}{L^\infty} \int _{-1}^1 \module{\phi'(t) \psi'(t)} \ dt + \norm{\gamma}{L^\infty}\int _{-1}^1 \module{\phi(t) \psi(t)} \ dt \\
 & \leq M \left ( \norm{\phi'}{L^2} \norm{\psi'}{L^2} + \norm{\phi}{L^2} \norm{\psi}{L^2} \right ) \\
 & \leq M \sqrt{\norm{\phi}{L^2}^2 + \norm{\phi'}{L^2}^2} \sqrt {\norm{\psi}{L^2}^2 + \norm{\psi'}{L^2}^2} \\
 & = M \norm{\phi}{H^1}\norm{\psi}{H^1}
\end{align*}
Ce qui montre que $a$ est une forme bilinéaire continue sur $H_0^1(]-1,1[)$.

Il reste donc a montrer que $a$ est coercive. Pour cela il suffit de voir que
\[
 a(\phi,\phi) = \int_{-1}^1 \alpha(t) \module{\phi'(t)}^2\ dt + \int_{-1}^1 \gamma(t) \module{\phi(t)}^2\ dt \geq \alpha_0 \norm{\phi'}{L^2}^2
\]
et par l'inégalité de Poincaré on sait que la norme $\phi \mapsto \norm{\phi'}{L^2}$ est équivalente à la norme usuelle de $H_0^1(]-1,1[)$.

On peut à présent appliquer le théorème de Lax-Milgram pour obtenir l'existence d'une unique solution $u$ à la formulation variationnelle du problème \eqref{eq:problem_3} dans $H_0^1(]-1,1[)$. Il nous faut à présent montrer que cette solution est dans $H_0^2(]-1,1[)$, c'est-à-dire qu'elle admet une dérivée faible seconde qui soit dans $L^2(]-1,1[)$. On cherche donc une fonction $g \in L^2(]-1,1[)$ qui vérifie
\[
 - \int u \phi' = \int g \phi
\]
pour toute fonction test $\phi \in \mathcal{D}(]-1,1[)$.

Comme $u$ est solution de la formulation variationnelle du problème, on sait déjà que
\[
 \int _{-1}^1 (\alpha(t)u'(t)) \phi'(t)\ dt = \int _{-1}^1 (f(t) - \gamma(t) u(t)) \phi(t) \ dt.
\]
Ce qui signifie que la fonction $\alpha u'$ admet comme dérivée faible $\gamma u -f$ qui se trouve être dans $L^2$. On a donc que $\alpha u \in H^1$. Or on sait que $(\alpha u')' = \alpha' u' + \alpha u''$. Ce qui donne
\[
 u'' = \frac{\gamma u - \alpha' u' - f}{\alpha}
\]
qui a bien un sens car $\alpha$ est supposée strictement positive.De plus peut voir que $u''$ est dans $L^2$. Ce qui montre que la solution $u$ de \eqref{eq:problem_3} est dans $H^2$.

\subsection{Question 5}

On se donne la fonction
\[
 \omega (x) = \frac{1}{\sqrt{1 - x^2}}
\]
et on cherche à montrer que la forme bilinéaire
\[
 \scalaire{\phi}{\psi}_\omega = \int_{-1}^1 \phi(x) \psi(x) \omega(x) \ dx
\]
définit un produit scalaire sur $\mathcal{C}^0([-1,1])$.

La forme bilinéaire $\scalaire{\cdot}{\cdot}\omega$ est clairement symétrique et on a
\begin{align*}
 \scalaire{\phi}{\phi}_\omega = \int \frac{\module{\phi(x)}^2}{\sqrt{1-x^2}} \ dx \geq 0
\end{align*}
et on remarque que $\scalaire{\phi}{\phi}_\omega = 0$ si, et seulement si, $\phi = 0$ car $\omega$ est strictement positive sur $]-1,1[$.
Il reste juste à montrer que l'intégrale est bien définie. Or on a
\begin{align*}
 \module{\scalaire{\phi}{\psi}_\omega} \leq \norm{\phi}{\infty} \norm{\psi}{\infty} \int_{-1}^1 \frac{1}{\sqrt{1 - x^2}}\ dx.
\end{align*}

Or on a
\begin{align*}
 \int_{-1}^1 \frac{1}{\sqrt{1 - x^2}}\ dx &= 2 \int_0^1 \frac{1}{\sqrt{1-x} \sqrt{1+x}}\ dx \\
 & \leq 2 \int_0^1 \frac{1}{\sqrt{1-x}}\ dx < + \infty.
\end{align*}
Ce qui montre que $\scalaire{\cdot}{\cdot}_\omega$ définit un produit scalaire.


\section{Polynômes de Tchebychev}

\subsection{Question 1}

 On montre que pour tout $n \in \N$ in existe une unique polynôme $T_n \in \R_n[X]$ tel que $T_n(\cos(\omega)) = \cos(n\omega)$. On procède par récurrence en commençant par remarquer que que l'on a clairement $T_0(X) = 1$ et $T_1(X) = X$. Ensuite on a
\begin{align*}
 (\cos(\omega))^n &= \left ( \frac{e^{i\omega} + e^{-i\omega}}{2} \right ) ^n = 2^{-n} \sum_{k=0}^n \binom{n}{k} e^{ik\omega}e^{-i(n-k)\omega} \\
 &= 2^{-n} \sum_{k=0}^n \binom{n}{k} e^{i(2k-n)\omega}.
\end{align*}

On note de plus que $\binom{n}{k} = \binom{n}{n-k} = \frac{n!}{k!(n-k)!}$ et on note alors cette quantité $h_k^n$. On a alors
\begin{align*}
 (\cos(\omega))^n &= 2^{-n} \sum_{k=0}^{\lfloor n/2 \rfloor } h_k^n \left ( e^{i(2k -n) \omega} + e^{i(n - 2k)\omega} \right) \\
 &= 2^{-(n-1)} \sum_{k=0}^{\lfloor n/2 \rfloor } h_k^n \cos((n - 2k)\omega)).
\end{align*}

On note à présent $m = \lfloor n/2 \rfloor$ et d'après l'hypothèse de récurrence pour tout $k \in \{1, \dots, m\}$ il existe un polynôme $T_{n-2k}(X)$ tel que $T_{n - 2k}(\cos(\omega)) = \cos((n - 2k)\omega)$. On peut alors poser
\[
 T_n(X) = 2^{(n-1)} X^n - \sum_{k=1}^m  \widetilde{h_k^n} T_{n-2k}(X).
\]
où on a noté $\widetilde{h_k^n} = \begin{cases}
                                   0 \quad \textnormal{si } h_k^n = 0 \\
                                   \frac{1}{h_k^n} \quad \textnormal{sinon}
                                  \end{cases}$.

Pour l'unicité, si on suppose qu'il existe un autre polynôme $R_n(X)$ vérifiant que $R_n(\cos(\omega)) = \cos(n\omega)$. Alors pour tout $\omega \in [0,2\pi]$ $\cos(\omega)$ est une racine du polynôme $T_n - R_n$ ce qui implique $T_n - R_n = 0$.

\begin{figure}
 \centering
 \includegraphics[width=0.8\textwidth]{tchebychev.png}
 \caption{Premiers polynômes de Tchebychev}
\end{figure}



\subsection{Question 2}

\paragraph{a.} On montre que $T_{n + 2}(x) + T_n(x) = 2xT_{n+1}(x)$. Pour cela on a
\begin{align*}
 T_{n + 2}(cos(\omega)) + T_n(\cos(\omega)) &= \cos((n+2)\omega) + \cos(n\omega) \\
 &= \frac{e^{i((n+2)\omega)} + e^{-i((n+2)\omega)} + e^{in\omega} + e^{in\omega}}{2} \\
 &= \frac{e^{i((n+1)\omega)}(e^{in\omega} + e^{-in\omega}) + e^{-i((n+1)\omega)}(e^{in\omega} + e^{-in\omega})} {2} \\
 &= 2\cos(\omega)\cos((n+1)\omega) \\
 &= 2\cos(\omega) T_{n+1}(\cos(\omega)).
\end{align*}
Comme de plus $\cos$ est une bijection de $[0,2\pi]$ dans $[-1,1]$ on a montré que pour tout $x \in [-1,1]$ on a $T_{n + 2}(x) + T_n(x) = 2xT_{n+1}(x)$.

\paragraph{b.} La relation $T_n(\cos(x)) = \cos(nx)$ nous donne nécessairement $T_0(x) = 1$ et $T_1 (x) = x$. En utilisant la question \emph{a.} on trouve alors 
\begin{align*}
 T_2(x) &= 2x T_1(x) - T_0(x) = 2x^2 - 1 \\
 T_3(x) &= 2xT_2(x) - T_1(x) = 2x(2x^2 -1) - x = 4x^3 - 3x.
\end{align*}

\paragraph{c.} On montre que pour tout $n \geq 2$ et pour tout $x \in [-1,1]$ on a 
\begin{equation}\label{eq:eq1}
 2 T_n(x) = \frac{1}{n+1} T_{n+1}'(x) - \frac{1}{n-1}T_{n-1}'(x).
\end{equation}


On a 
\begin{align*}
 T_{n+1}' &= \frac{n+1}{\sqrt{1-x^2}}\sin((n-1)\arccos(c)) \quad \textnormal{et} \\
 T_{n-1}'(x) &= \frac{n-1}{\sqrt{1 - x^2}}\sin((n-1)\arccos(x)),
\end{align*}
on a donc 
\begin{align*}
 \frac{1}{n+1}T_{n+1}'(x) &- \frac{1}{n-1}T_{n-1}'(x) = \frac{1}{\sqrt{1-x^2}} \left[ \sin((n+1)\arccos(x)) - \sin((n-1)\arccos(x)) \right ] \\
 &= \frac{1}{\sqrt{1-x^2}}  [ \sin(n \arccos(x))x + \sqrt{1-x^2} \cos(n\arccos(x)) \\ & \quad - \sin(n\arccos(x))x + \sqrt{1-x^2} \cos(n\arccos(x))  ] \\
 &= 2 \cos (n\arccos(x)) = 2 T_n(x)
\end{align*}
Ce qui montre la relation \eqref{eq:eq1}.

\paragraph{d.} On montre les deux affirmations simultanément par récurrence. On vérifie facilement que l'on a $T_1'(x) = 1 = T_0(x) = 2\sum^\star T_k(x)$, $T_2'(x) = 4x = 4T_1(x) = 4 \sum^\star T_k$ et $T_2''(x) = 4 = 8 \sum^\star T_k$, ce qui initialise la récurrence.

On suppose à présent $n>2$ et que le résultat est vrai pour tout $k<n$. On a alors 
\begin{align*}
 2 T_{n-1}(x) &= \frac{1}{n} T_n'(x) - \frac{1}{n-2}T_{n-2}(x), \quad \textnormal{alors} \\
  \frac{1}{n} T_n'(x) &= 2T_{n-1}(x) + \frac{1}{n-2}T_{n-2}(x), \quad \textnormal{ce qui donne} \\ 
 T_n'(x) &= 2nT_{n-1}(x) + \frac{n}{n-2}T_{n-2}'(x) \\
 &= 2n T_{n-1}(x) + \frac{n}{n-2}\ 2(n-2) \sum_{k=0}^{n-3} \phantom{}^\star T_k \\
 &= 2n \sum_{k=0}^{n-1} \phantom{}^\star T_k(x).
\end{align*}

A noter que l'on a inclus la condition de parité dans la définition du symbole $\sum^\star$. On a donc montré la première relation. Pour la seconde on a 

Pour la seconde relation on a 
\begin{align*}
 T_n''(x) &= 2n T_{n-1}'(x) + \frac{n}{n-2} T_{n-2}''(x) \\
 &= 4n(n-1) \sum_{k=0}^{n-2} \phantom{}^\star T_k(x) + n \sum_{k=0}^{n-4} \phantom{}^\star ((n-4)^2 - k^2) T_k(x) \\
 &= n \left [ 4(n-1) \sum_{k=0}^{n-2} \phantom{}^\star T_k(x) + \sum_{k=0}^{n-4} \phantom{}^\star ((n-2)^2 - k^2) T_k(x) \right ] \\
 &= n \left [ (4n-4) T_{n-2}(x) + \sum_{k=0}^{n-4} \phantom{}^\star (4(n-1) + (n-2)^2 - k^2) T_k(x) \right ] \\
 &= n \left [ (n^2 - (n-2)^2) T_{n-2}(x) + \sum_{k=0}^{n-4} \phantom{}^\star (n^2 - k^2) T_k(x) \right ] \\
 &= n \sum_{k=0}^{n-2} \phantom{}^\star (n^2 - k^2)T_k(x).
\end{align*}
Ce qu'il fallait montrer.

\subsection{Question 3}

\paragraph{a.} Pour tout $n$ on a $T_n(\cos(x)) = \cos(nx)$. Alors en posant $\theta_k = \frac{2k + 1}{2n}\pi$ pour $k \in \{0, \dots, n-1\}$ on a $\cos(n \theta_k) = \cos \left ( \frac{(2k+1)\pi}{2}\right) = 0$ et donc $T_n(\cos(\theta_k)) = 0$. Alors la famille $\{\cos(\theta_k)\}_{k \in \{0, \dots, n-1\}}$ donne $n$ racines distinctes de $\widetilde{T_n} = 2^{-(n-1)} T_n$ qui est unitaire. Par le théorème fondamentale de l'algèbre on sait qu'il ne peut pas exister d'autre racine du polynôme $\widetilde{T_n}$ et on a de plus la factorisation
\[
 T_n(x) = 2^{n-1}\prod_{k=0}^{n-1}(x - \cos(\theta_k)).
\]

\paragraph{b.} A partir de la relation $T_{n+2}(x) = 2xT_{n+1}(x) - T_n(x)$ on trouve facilement que $T_n(1) = 1$ et $T_n(-1) = (-1)^n$. De plus on avait déjà que le coefficient dominant était $2^{n-1}$.

\subsection{Question 4}

On a 
\begin{align*}
 \scalaire{T_n}{T_m}_\omega &= \int_{-1}^1 \frac{T_n(x) T_m(x)}{\sqrt{1 - x^2}}\ dx  = \int_{0}^{\pi} T_n(\cos(\xi)) T_m(\cos(\xi))\ d\xi \\
 &= \int_{0}^{\pi} \cos(n\xi)\cos(m\xi)\ d\xi = \begin{cases}
                                                 \pi \quad \textnormal{si } n=m = 0 \\
                                                 \frac{\pi}{2} \quad \textnormal{si } n=m \neq 0 \\
                                                 0 \quad \textnormal{ sinon}
                                                \end{cases}.
\end{align*}
Ce qui montre que la famille est orthogonale pour le produit scalaire $\scalaire{\cdot}{\cdot}_\omega$. De plus la famille est constituée de $n+1$ polynômes indépendants (car de degrés différents) dans un espace de dimension $n+1$. Il s'agit donc bien d'une base orthogonale.

\section{Méthode des rectangles pour l’approximation de l’intégrale d’une fonction périodique sur un intervalle de période}

On rappelle ici le r\'esultat suivant (utilis\'e \`a plusieurs reprises par la suite) :
En consid\'erant une fonction $f$ de $\mathbb{R}$ dans $\mathbb{R}$, p\'eriodique de p\'eriode $2 \pi$ assez r\'eguli\`ere. Alors la s\'erie de fourier associ\'ee \`a $f$, not\'ee $S_Nf$ converge normalement (et donc uniform\'ement) vers $f$ sur $[0, 2\pi]$ lorsque $N$ tend vers l'infini

\subsection{Question 1}

Soient $n$ et $k$ deux entiers naturels tels que $n \geq 2$ et $k \geq 2$.\\.
On pose $k=pn+r$, avec $0 < r \leq n-1$, la division euclidienne de $k$ par $n$. Montrons que $S_{n,k} = 0$ si $r=0$ et  $S_{n,k} = n$ sinon :
\[
S_{n,k} = \sum_{j=0}^{n-1} \cos\left(\frac{2kj\pi}{n}\right)
= \sum_{j=0}^{n-1} \cos\left(\frac{2pnj\pi}{n}+\frac{2rj\pi}{n}\right)
= \sum_{j=0}^{n-1} \cos\left(2pj\pi+\frac{2rj\pi}{n} \right)
\]
la fonction cosinus \'etant p\'eriodique de p\'eriode $2\pi$, on a donc :
\[
S_{n,k} = \sum_{j=0}^{n-1} \cos\left(\frac{2rj\pi}{n} \right)
\]
Si $r=0$, on a directement le r\'esultat :
\[
S_{n,k} = \sum_{j=0}^{n-1} \cos\left(\frac{2\pi}{n} \right)
= \sum_{j=0}^{n-1} 1
= n
\]
Sinon, on utilise la formule d'Euler pour le cosinus :
\[
\cos(x)= \Re\left( {\mathrm  {e}}^{{{\mathrm  {i}}\,x}} \right) 
\]
O\`u $\Re$ d\'esigne la partie r\'elle d'un complexe.
Ce qui nous donne :
\[
S_{n,k} = \sum_{j=0}^{n-1} \Re\left( {\mathrm  {e}}^{{{\mathrm  {i}}\,\frac{2rj\pi}{n}}} \right)
=\Re\left( \sum_{j=0}^{n-1} {\mathrm  {e}}^{{{\mathrm  {i}}\,\frac{2rj\pi}{n}}} \right)
\]
Or, les $\mathrm{e}^{{{\mathrm  {i}}\,\frac{2rj\pi}{n}}}$ sont les n-i\`emes racines de l'unit\'e. On a donc la propri\'et\'e que leur somme vaut $0$. Ceci nous permet de conclure :
\[
S_{n,k} =\Re\left( \sum_{j=0}^{n-1} {\mathrm  {e}}^{{{\mathrm  {i}}\,\frac{2rj\pi}{n}}} \right)
=\Re\left( 0 \right)
=0
\]

\subsection{Question 2}
\subsubsection {Partie a}

Soit $\left(x_j\right)_{0 \leq j \leq n} $ une subdivision uniforme de l'intervalle $[0, \pi]$. Notons h le pas de la subdivision.\\
On a par d\'efinition de la formule de quadrature des rectangles \`a gauche et d'apr\`es le d\'evelopppement en s\'erie de $f$ :
\[
I_n = \sum_{j=0}^{n-1} f(x_j) \cdot h
= \frac{2 \pi}{n} \sum_{j=0}^{n-1} f(x_j)
= \frac{2 \pi}{n} \sum_{j=0}^{n-1} \left( a_0 + \sum_{k=1}^{+ \infty} a_k \cos(k x_j)\right)
\]
Puisque la s\'erie converge, on peut intervertir les sommes :
\[
I_n = \frac{2 \pi}{n} \cdot n a_0 + \frac{2 \pi}{n} \sum_{k=1}^{+ \infty} \left( \sum_{j=0}^{n-1} a_k \cos(k x_j) \right)
= 2 \pi a_0 + \frac{2 \pi}{n} \sum_{k=1}^{+ \infty} \left( a_k \sum_{j=0}^{n-1} \cos(k x_j) \right)
\]
En remarquant que les $x_j = \frac{2 \pi j}{n}$ on a donc :
\[
I_n = 2 \pi a_0 + \frac{2 \pi}{n} \sum_{k=1}^{+ \infty} \left( \sum_{j=0}^{n-1} a_k \cos\left( \frac{2 k j \pi }{n} \right) \right)
= 2 \pi a_0 + \frac{2 \pi}{n} \sum_{k=1}^{+ \infty} a_k S_{n,k}
\]
Ce qui conclut cette question.

\subsubsection{Partie b}

Posons $I= \int_{0}^{2 \pi} f(x) dx$. On s'int\'eresse \`a l'erreur $I_n - I$. On a d'un part :
\begin{align*}
I = &\int_{0}^{2 \pi} f(x) = \int_{0}^{2 \pi} \sum_{k=0}^{+ \infty} a_k \cos(kx) dx
= \int_{0}^{2 \pi} a_0 + \sum_{k=1}^{+ \infty} a_k \cos(kx) dx \\
=& \int_{0}^{2 \pi} a_0 dx + \int_{0}^{2 \pi} \sum_{k=}^{+ \infty} a_k \cos(kx) dx
= 2 \pi a_0 + \sum_{k=1}^{+ \infty} \int_{0}^{2 \pi} a_k \cos(kx) dx\\
=& 2 \pi a_0 + \sum_{k=1}^{+ \infty} {\left[ \frac{\sin(kx)}{k} \right]}_0^{2\pi}
= 2 \pi a_0 + \sum_{k=1}^{+ \infty} 0 = 2 \pi a_0 + 0 = 2 \pi a_0
\end{align*}
L'interversion s\'erie int\'egrale est justifi\'ee car la s\'erie de Fourier converge uniform\'ement vers $f$. \\
D'autre part, on a par la question pr\'ec\'edente (et puisque la s\'erie converge) :
\[
\sum_{k=1}^{+ \infty} a_k S_{n,k} = \sum_{p=1}^{+ \infty} a_{pn} S_{n,pn}
= \sum_{p=1}^{+ \infty} a_{pn} \cdot n
= n \cdot \sum_{p=1}^{+ \infty} a_{pn}
\]
On en d\'eduit donc :
\begin{align*}
I_n - I =& I_n -  2 \pi a_0 = 2 \pi a_0 + \frac{2 \pi}{n} n \cdot \sum_{p=1}^{+ \infty} a_{pn} -  2 \pi a_0\\
=& 2 \pi \sum_{p=1}^{+ \infty} a_{pn}
\end{align*}
Ce qui conclut.

\subsubsection{Partie c}
On s'int\'eresse au comportement de la quantit\'e $I_n-I$ lorsque n tend vers l'infini. On peut encore une fois utiliser la convergence uniforme de la s\'erie de Fourier pour l'intervertion limite-s\'erie.

\begin{align*}
\lim_{n\to\infty} I_n-I  &=  \lim_{n\to\infty} 2 \pi \sum_{p=1}^{+ \infty} a_{pn} \\
&\leq \lim_{n\to\infty} 2 \pi  \sum_{p=1}^{+ \infty} a_{pn} \\
&\leq \lim_{n\to\infty} 2 \pi \sum_{p=1}^{+ \infty}   a_{pn} \\
\end{align*}

On a : 
\begin{align*}
\lim_{n\to\infty}  I_n-I  &=2 \pi \sum_{p=1}^{+ \infty} \lim_{n\to\infty}  a_{pn}  \\
&= 2 \pi \sum_{p=1}^{+ \infty} 0 = 0
\end{align*}

\subsection{Question 3}

Supposons que $f$ est de classe $\C^q$ (et v\'erifiant les autres hypoth`eses de l'\'enonc\'e. Les $a_k$ v\'erifient donc $a_k=\frac{\alpha_k}{n^q}$, o\`u $\alpha_k$ est une suite r\'elle tendant vers $0$ lorsque $k$ tend vers l'infini.
On a alors en particulier qu'il existe M r\'eel positif tel que : $|\alpha_k| \leq M$.\\
Cela nous donne que quel que soient $p$ et $n$ entiers positifs, on a : $|a_{pn}| \leq \frac{M}{(pn)^q}$
Et donc :
\begin{align*}
I_n - I =& 2 \pi \sum_{p=1}^{+ \infty} a_{pn}\\
\leq& 2 \pi \sum_{p=1}^{+ \infty} \frac{M}{(pn)^q}\\
\leq& \frac {2 \pi}{n^q}\sum_{p=1}^{+ \infty} \frac{M}{p^q}\\
\end{align*}
Puisque la s\'erie converge (s\'erie de Riemann) on a bien qu'il existe une constante r\'elle C telle que : $|I_n - I| \leq \frac{C}{n^q}$.

On peut interpr\'eter ce r\'esultat de la mani\`ere suivant : l'ordre de convergence de la m\'ethode des rectangles pour le type de fonction que nous \'etudions (paires et p\'eriodiques sur $[0;2 \pi]$) est \'egal \`a l'ordre $q$ de la classe de r\'egulartit\'e de la fonction \'etudi\'ee.


\section{Système linéaire issu de la méthode des résidus pondérés}

On considère le problème : trouver une fonction $u$ telle que 
\begin{align*}
 - \alpha u''(x) + \gamma u(x) &= f(x) \quad \forall x \in ]-1,1[, \\
 u(-1) = u(1) &= 0.
\end{align*}

Pour tout $k \in \N$ on désigne pas $\phi_k$ la fonction définie sur $]-1,1[$ par 
\begin{align*}
 \phi_k(x) = \begin{cases}
              T_{k+2}(x) - T_0(x) \quad \textnormal{si $k$ est pair},\\
              T_{k+2}(x) - T_1(x) \quad \textnormal{si $k$ est impair}
             \end{cases}.
\end{align*}



\subsection{Question 1}

On montre que la famille $\Phi = \{ \phi_0, \dots, \phi_{N-2}\}$ est une base de l'espace vectoriel 
\[
 V_N = \{v \in \R_N[X] : v(-1) = v(1) = 0 \}.
\]

$V_N$ est un sous-espace de l'espace vectoriel des polynômes de degrés compris entre $2$ et $N$, en effet un polynôme de $V_N$ dont admettre au moins deux racines. Cet espace est donc de dimension au plus $N-1$. Or la famille $\Phi$ contient $N-1$ polynômes linéairement indépendants de $V_N$, car ils sont tous de degrés différents et que $\phi_k(1) = \phi_k(-1) = 0$ d'après la question 2.3.b, elle est donc libre maximale dans $V_N$, c'est donc une base de $V_N$.

\subsection{Question 2}

On a l'opérateur différentiel $ \mathcal{L}u = -\alpha u'' + \gamma u$. On cherche $\widehat{u_N} \in V_N$ tel que pour tout $v_N \in V_N$ on ait
\[
 \scalaireo{\mathcal{L}[\widehat{u_N}]}{v_N} = \scalaireo{f}{v_N}.
\]

Ce qui revient à 
\[
 \int_{-1}^{1} \frac{(- (\alpha \widehat{u_N}')' + \gamma \widehat{u_N})v_N}{\sqrt{1 - x^2}}\ dx = \int_{-1}^1 \frac{f v_N}{\sqrt{1 - x^2}},
\]
or comme la fonction $x \mapsto \frac{1}{\sqrt{1 - x^2}}$ est une fonction strictement positive sur $]-1,1[$, cela revient à 
\[
 \int_{-1}^1 - (\alpha \widehat{u_N}')'v_N\ dx + \int_{-1}^1 \gamma \widehat{u_N}v_N \ dx = \int_{-1}^1 f v_N \ dx
\]
et en intégrant par partie on trouve 
\[
 \int_{-1}^1 \alpha \widehat{u_N}'v_N\ dx + \int_{-1}^1 \gamma \widehat{u_N}v_N \ dx = \int_{-1}^1 f v_N \ dx
\]
car $v_N(-1) = v_N(1) = 0$. On retrouve bien une formulation variationnelle exprimée dans un espace vectoriel de dimension finie. D'où le parallèle avec la méthode de Galerkine.

\subsection{Question 3}
On commence par utiliser la question 2.4 pour déterminer la valeur de $\scalaireo{\phi_l}{\phi_k}$. 
\begin{itemize}
 \item Si $k = l$ est pair, on a 
 \[
  \scalaireo{\phi_k}{\phi_k} = \scalaireo{T_{k+2}}{T_{k+2}} + \scalaireo{T_0}{T_0} = \frac{\pi}{2} + \pi  = \frac{3\pi}{2}.
 \]
 
 \item Si $k=l$ est impaire :
 \[
  \scalaireo{\phi_k}{\phi_k} = \scalaireo{T_{k+2}}{T_{k+2}} + \scalaireo{T_1}{T_1} = \frac{\pi}{2} + \frac{\pi}{2}  = \pi.
 \]
 
 \item Si $k \neq l$ tous deux pairs :
 \[
  \scalaireo{\phi_l}{\phi_k} = \scalaireo{T_0}{T_0} = \pi.
 \]
 
 \item Si $k \neq l$ tous deux impairs :
 \[
  \scalaireo{\phi_l}{\phi_k} = \scalaireo{T_0}{T_0} = \frac{\pi}{2}.
 \]

 \item Enfin si $k$ et $l$ sont de parité opposée on a clairement 
 \[
  \scalaireo{\phi_k}{\phi_l} = 0.
 \]
 \end{itemize}
 On a bien trouvé les valeurs recherchées.
 
 On a de plus $\phi_k'' = T_{k+2}'' = (k+2) \sum_{i=0}^{k \star} ((k+2)^2 - i^2)T_i$. Ce qui nous permet de déterminer la valeur de $\scalaireo{\phi_l''}{\phi_k}$.
 \begin{itemize}
  \item Si $k$ et $l$ sont pairs et $0 \leq k \leq l-2$ on a 
  \begin{align*}
   \scalaireo{\phi_l''}{\phi_k} &= \scalaireo{T_{l+2}''}{T_{k+2} - T_0} = (l+2) \sum_{i=0}^{l} \phantom{}^\star ((l+2)^2 - i^2) \left ( \scalaireo{T_i}{T_{k+2}} - \scalaireo{T_i}{T_0} \right ) \\
   &= (l+2) \left ( (l+2)^2 - (k+2)^2 \right ) \scalaireo{T_{k+2}}{T_{k+2}} - (l+2)^2 \scalaireo{\frac{T_0}{2}}{T_0} \\
   &= (l+2) \left ( (l+2)^2 - (k+2)^2 \right ) \frac{\pi}{2} - (l+2)^2 \frac{\pi}{2} \\
   &= -(l+2)(k+2)^2 \frac{\pi}{2}.
  \end{align*}
  
  \item Si $k$ et $l$ sont pairs et $ 0 \leq l-2 < k$ :
  \begin{align*}
   \scalaireo{\phi_l''}{\phi_k} &= \scalaireo{T_{l+2}''}{T_{k+2} - T_0} = (l+2) \sum_{i=0}^{l} \phantom{}^\star ((l+2)^2 - i^2) \left ( \scalaireo{T_i}{T_{k+2}} - \scalaireo{T_i}{T_0} \right ) \\
   &= -(l+2) (l+2)^2 \scalaireo{\frac{T_0}{2}}{T_0} \\
   &= - (l+2)^3 \frac{\pi}{2}.
  \end{align*}

  \item Si $k$ et $l$ sont impairs et $0 \leq k \leq l-2$ :
  \begin{align*}
   \scalaireo{\phi_l''}{\phi_k} &= \scalaireo{T_{l+2}''}{T_{k+2} - T_1} = (l+2) \sum_{i=0}^{l} \phantom{}^\star ((l+2)^2 - i^2) \left ( \scalaireo{T_i}{T_{k+2}} - \scalaireo{T_i}{T_1} \right ) \\
   &= (l+2) \left ( (l+2)^2 - (k+2)^2 \right ) \scalaireo{T_{k+2}}{T_{k+2}} - ((l+2)^2 - 1) \scalaireo{T_1}{T_1} \\
   &= (l+2) \left ( (l+2)^2 - (k+2)^2 \right ) \frac{\pi}{2} - ((l+2)^2 - 1) \frac{\pi}{2} \\
   &= -(l+2)((k+2)^2 + 1) \frac{\pi}{2}.
  \end{align*}

  \item  Si $k$ et $l$ sont impairs et $ 0 \leq l-2 < k$ :
  \begin{align*}
   \scalaireo{\phi_l''}{\phi_k} &= \scalaireo{T_{l+2}''}{T_{k+2} - T_1} = (l+2) \sum_{i=0}^{l} \phantom{}^\star ((l+2)^2 - i^2) \left ( \scalaireo{T_i}{T_{k+2}} - \scalaireo{T_i}{T_1} \right ) \\
   &= -(l+2) ((l+2)^2 - 1) \scalaireo{T_1}{T_1} \\
   &= -(l+2) ((l+2)^2 - 1) \frac{\pi}{2}.
  \end{align*}

  \item Enfin il est facile de voir que si $k$ $l$ sont de parité opposée on a $\scalaireo{\phi_l''}{\phi_k} = 0$.
 \end{itemize}
 
 \begin{figure}
 \centering
 \includegraphics[width=0.8\textwidth]{phi.png}
 \caption{Premières fonction $\phi_i$}
\end{figure}
 
 \subsection{Question 4}
 On considère une fonction $f \in L^2([-1,1])$, on s'intéresse à $\scalaireo{f}{T_n}$.
 
 \paragraph{a.} On a 
 \begin{align*}
  \scalaireo{f}{T_n} &= \int_{-1}^1 \frac{f(x)T_n(x)}{\sqrt{1 - x^2}}\ dx = \int_0^\pi \frac{f(\cos(t))T_n(\cos(t))}{\sqrt{1 - \cos^2(t)}} \sin(t)\ dt \\ 
  &= \int_0^\pi f(\cos(t))\cos(nt)\ dt = \frac{1}{2} \int_0^{2\pi} f(\cos(t))\cos(nt)\ dt.
 \end{align*}
 Où l'on a commencé par faire le changement de variable $x = \cos(t)$ puis on a utilisé la parité de la fonction $t \mapsto \cos(t)$.
 
 \paragraph{b.} La méthode des rectangles donne 
 \begin{align*}
  I_n &= \frac{2\pi}{2(N+1)} \sum_{j=0}^N f\left(\cos\left(\frac{2j\pi}{N+1}\right) \right) \cos \left (\frac{n 2 \pi j}{N+1} \right ) \\
  &= \frac{\pi}{N+1} \sum_{j=0}^N f \left ( \cos \left (\frac{2j\pi}{N+1} \right) \right ) \Re \left [e^{-i \frac{2n\pi j}{N+1}} \right ]\\
  &= \frac{\pi}{N+1} \sum_{j=0}^N w_j \Re \left[e^{-2i\pi \frac{nj}{N+1}} \right ] \\
  &= \frac{\pi}{N+1} \Re \left [ \sum_{j=0}^N w_j e^{-2i\pi \frac{nj}{N+1}} \right ].
 \end{align*}

 \paragraph{c.} Dans le cas pair on a 
 \begin{align*}
  \scalaireo{f}{\phi_n} &= \scalaireo{f}{T_{n+2}} - \scalaireo{f}{T_0} \\
  &= \frac{\pi}{N+1} \Re \left [ \sum_{j=0}^N w_j \left ( e^{-2i\pi \frac{(n+2)j}{N+1}} - 1 \right) \right ].
 \end{align*}
 
 Dans le cas impaire on a 
\begin{align*}
 \scalaireo{f}{\phi_n} &= \scalaireo{f}{T_{n+2}} - \scalaireo{f}{T_1} \\
  &= \frac{\pi}{N+1} \Re \left [ \sum_{j=0}^N w_j  e^{-2i\pi \frac{j}{N+1}}\left ( e^{n+2} -1 \right ) \right ].
\end{align*}


\section{Transform\'ee de Fourier rapide}

\subsection{Question 2.a}

Soit $p$ et $n$ deux entiers naturels tels que $p > 0$. On calcule les valeurs exactes des int\'egrales de la mani\`ere suivante : (on suppose premi\`erement $n$ strictement positif)

\begin{align*}
 \langle f,T_n \rangle _\omega &= \frac{1}{2} \int_0^{2\pi} f(\cos(t))\cos(nt)\ dt
 = \frac{1}{2} \int_0^{2\pi} t^p (2\pi - t)^p\cos(nt)\ dt\\
 &= \frac{1}{2} \int_0^{2\pi} \left[t (2\pi - t)\right]^p\cos(nt)\ dt
\end{align*}

On effetue une IPP avec $u=\left[t (2\pi - t)\right]^p$ et $v'=\cos(nt)$.\\
On a donc $u'=2p(pi-t)\left[t (2\pi - t)\right]^(p-1)$ et $v=\frac{1}{n} \sin(nt)$. Ainsi :

\begin{align*}
 \langle f,T_n \rangle _\omega &= \frac{1}{2} \left[ \left[t (2\pi - t)\right]^p \frac{1}{n} \sin(nt) \right]_0 ^{2\pi} + \frac{1}{2} \int_0^{2\pi}  \frac{1}{n} 2p(\pi-t)\left[t (2\pi - t)\right]^{p-1} \sin(nt) \ dt \\
 &= \frac{p}{n} \int_0^{2\pi} (\pi-t)\left[t (2\pi - t)\right]^{p-1} \sin(nt) \ dt
\end{align*}

Dans le cas ou $p=1$, on a alors :

\begin{align*}
 \langle f,T_0 \rangle _\omega &=  \frac{1}{n} \int_0^{2\pi} (\pi-t) \sin(nt) \ dt\\
 &=\frac{1}{n^2} \left[(\pi -t)  \cos(nt) \right]_0 ^{2\pi} + \frac{1}{n^2} \int_0^{2\pi} \cos(nt) \ dt\\
 &= - \frac{\pi}{n^2} + \frac{1}{n^3} \int_0^{2\pi n} \cos(\phi) \ d\phi\\
 &= - \frac{\pi}{n^2}
\end{align*}

Lorsque $n=0$, on a a :
\begin{align*}
 \langle f,T_n \rangle _\omega &=  \frac{1}{2} \int_0^{2\pi} f(\cos(t))\ dt
 =\frac{1}{2} \int_0^{2\pi} t^p (2\pi - t)^p \ dt 
 =\frac{1}{2} \int_0^{2\pi} \left( 2 \pi t - t^2 \right)^p \ dt\\
 &= \frac{1}{2} \int_0^{2\pi} \left( 2 \pi t - t^2 \right) \ dt
 = \left[\pi t^2 - \frac{t^3}{3}  \right]_0 ^{2\pi}
 = \frac{2 \pi^3}{3}
\end{align*}

Dans le cas ou $p=4$, on a :
\begin{align*}
 \langle f,T_n \rangle _\omega &= \frac{p}{n} \int_0^{2\pi} (pi-t)\left[t (2\pi - t)\right]^{3} \sin(nt) \ dt
\end{align*}

Lorsque $n=0$, on a a :
\begin{align*}
 \langle f,T_0 \rangle _\omega &=  \frac{1}{2} \int_0^{2\pi} f(\cos(t))\ dt
 =\frac{1}{2} \int_0^{2\pi} t^4 (2\pi - t)^4 \ dt 
 =\frac{1}{2} \int_0^{2\pi} \left( 2 \pi t - t^2 \right)^4 \ dt\\
\end{align*}

\begin{figure}
 \includegraphics[width=0.8\textwidth]{pswft.png}
 \caption{Valeurs de produits scalaires pour $p = 1$}
\end{figure}

\begin{figure}
 \includegraphics[width=0.8\textwidth]{err.png}
 \caption{Erreur relative pour $p = 1$}
\end{figure}




\section{Méthode de Gauss}

\subsection{Question 1}

Pour chaque ligne $j > 1$, et en partant la dernière ligne $j = n$, l'idée est de soustraire la ligne $j-1$ à la ligne $j$. La structure particulière de la matrice nous permet de ne pas réaliser la totalité des calculs. Pour chaque ligne $j$ on peut directement fixer les $j-2$ premiers coefficients à $0$ et il ne reste plus qu'à réaliser $n - j +2$ soustractions. Cette méthode donne une matrice de la forme souhaitée en réalisant $\sum_{k=1}^n (n - k + 2) = \sum_{k=0}^{n-1} (k + 2) \sim n^2/2$ soustractions.

\subsection{Question 2}

Pour chaque colonne $c_k$ pour $k>1$ et en partant de la colonne la plus à droite $k = n$, on élimine le coefficient $\hat {b}_{k-1}$ en calculant
\[
 c_{k-1} = c_{k-1} - \frac{\hat {b}_{k-1}}{\hat{a}_{k,k}} c_k.
\]
Ce qui donne lieu à $3n(n-1)$ opérations. On obtient alors un matrice triangulaire supérieure qui nous permet de résoudre le système linéaire en $n^2$ opérations comme dans le cas de la méthode de Gauss classique. Au final on a donc bien résolu le système linéaire avec un algorithme demandant de l'ordre de $n^2$ opérations.

\begin{remarque}
 Il faut remarquer que nous avons fait l'hypothèse que le coefficients diagonaux restaient non-nuls tout au long de la procédure.
\end{remarque}

\section{Mise en œuvre numérique}

 On sait déjà que l'on a le système linéaire $AU = F$ où $A = (a_{i,j})_{0 \leq i,j \leq N-2}$ et $a_{i,j} = 0$ dès que $i$ et $j$ sont de parité opposée. De plus en notant $F = (f_i)_{0 \leq i \leq N-2}$ on a avec les conventions de l'énoncé
\[
 f_i = \sum_{k = 0}^K (\alpha \scalaire{\phi_{2k}''}{\phi_i} - \gamma \scalaire{\phi_{2k}}{\phi_i})u_{2k}
\]
lorsque $i$ est pair et 
\[
 f_i = \sum_{k=0}^K (\alpha \scalaire{\phi_{2k+1}''}{\phi_i} - \gamma \scalaire{\phi_{2k+1}}{\phi_i})u_{2k+1}
\]
lorsque $i$ est impair. Alors en notant $G = \begin{pmatrix} G^1 \\ G^2                         \end{pmatrix}$ avec $G^1 = \begin{pmatrix}f_0 \\ f_2 \\ \vdots \\ f_{2K} \end{pmatrix}$ et $G^2 = \begin{pmatrix}f_1 \\ f_3 \\ \vdots \\ f_{2K+1} \end{pmatrix}$. On trouve bien un système équivalent $MV = G$ où $M$ est de la forme souhaitée.

Nous avons testé la méthode sur le problème suivant : trouver $u$ telle que 
\begin{align*}
 \begin{cases}
  -u''(x) + u(x) = x \quad \forall x \in ]-1,1[, \\
  u(-1) = u(1) = 0.
 \end{cases}
\end{align*}
Nous avons déterminé la solution exacte de ce problème par une méthode de tir et comparé la performances de cette méthode avec la résolution par méthode d'Euler combinée à la méthode de tir. La forme générale de la solution semble correcte mais il reste manifestement un problème de normalisation que nous n'avons pas su résoudre. Les fonctions Matlab/Octave sont reproduites en annexe.

\begin{figure}
 \centering
 \includegraphics[width=0.8\textwidth]{sol.png}
 \caption{Résulats obtenus pour le problème $-u'' + u = x$}
\end{figure}

\appendix

\section{Fonctions Matlab/Octave}

\begin{lstlisting}[language=Matlab]
 function I=pswft(f,N)
  
  % f : fonction dont on veut calculer le produit scalaire avec les polyomes 
  % de Tchebychev
  % N : nombre de produits scalaires calcules
  %
  % I : vecteur contenant les valeurs successives des produits scalaires
 
  tt=linspace(0,2*pi,N);
  w=f(tt);
  fft=fft(w);
  m=length(tt);
  I=pi*real(fft)./N;
endfunction
\end{lstlisting}


\hrule

\begin{lstlisting}[language=Matlab]
 function I = pswft2(f,N)
  % f : fonction dont on veut calculer le produit scalaire avec les polyomes
  % de Tchebychev
  % N : nombre de produits scalaires calcules
  %
  % I : vecteur contenant les valeurs successives des produits scalaires
  fcos = @(x)(f(cos(x)));
  I = pswft(fcos,N);
endfunction
\end{lstlisting}

\hrule 

\begin{lstlisting}[language=Matlab]
 function U = gauss(A,F)
  % Resout le systeme lineaire AU = F en utilisant la methode de Gauss 
  % decrite dans l'exercice 6
  N = size(A)(1);
  % On realise les operations sur les lignes
  for k = [N:-1:2]
    A(k,:) -= A(k-1,:);
    F(k) -= F(k-1);
  endfor
  
  % On realise les operations sur les colonnes
  lams = zeros(1,length([N:-1:2]));
  for k = [N:-1:2]
    lam = A(k,k-1)/A(k,k);
    lams(k-1) = lam;
    A(:,k-1) -= lam .* A(:,k);
  endfor
  U = solve_gauss(A,F);
  U = transpose(U);
endfunction
\end{lstlisting}

\hrule 

\begin{lstlisting}[language=Matlab]
 function U = solve_gauss(A,F)
  N = size(A)(1);
  U = zeros(1,N);
  U(N) = F(N)/A(N,N);
  for k = [N-1:-1:1]
    U(k) = (F(k) - sum(A(k,k+1:end).*U(k+1:end)))/A(k,k);
  endfor
endfunction
\end{lstlisting}

\hrule 

\begin{lstlisting}[language=Matlab]
 function U = reorder(V)
  l = length(V);
  U = zeros(l,1);
  K = floor(l/2);
  for k = [1:K]
    U(2*k-1) = V(k);
    U(2*k) = V(K+k);
  endfor
endfunction
\end{lstlisting}

\hrule 

\begin{lstlisting}[language=Matlab]
% Retourne les N premiers polynomes de tchebychev calcules sur x
function T = tchebychev(N,x)
  l = length(x);
  T = zeros(N,l);
  T(1,:) = ones(1,l);
  T(2,:) = x;
  for k = [3:N]
    T(k,:) = cos((k-1)*acos(x)); 
  endfor
endfunction 
\end{lstlisting}

\hrule 

\begin{lstlisting}[language=Matlab]
 function [M1,M2,G1,G2] = syslin(alpha,gamma,f,N)
  % On commence par creer le systeme lineaire pour lequel on 
  % n'a pas pris en compte la parite
  A = zeros(N,N);
  B = zeros(N,N);
  F = zeros(N,1);
  ps = pswft2(f,max(80,N+2));
  display(num2str(size(ps)));
  for l = [0:N-1]
    for k = [0:N-1]
      if mod(l,2) == mod(k,2)
        if mod(l,2) == 0
          if l==k
            A(l+1,k+1) += 3*pi/2;
          else 
            A(l+1,k+1) += pi;
          endif
          if l < k
            B(l+1,k+1) += -pi/2 * l**2 * k;
          else
            B(l+1,k+1) += -pi/2 * k**3;
          endif
        else
          if l==k
            A(l+1,k+1) += pi;
          else 
            A(l+1,k+1) += pi/2;
          endif
          if l < k
            B(l+1,k+1) += -pi/2 * (l**2 - 1) * k;
          else
            B(l+1,k+1) += -pi/2 * (k**2 -1)*k;
          endif
        endif
      endif
    endfor
    %F(l+1) = pswft(f,l+2,200) - pswft2(f,mod(l,2),200);
    F(l+1) = ps(l+3) - ps(mod(l,2)+1);
    j = mod(l,2);
    Tl = @(t)(cos((l+2)*t));
    Tj = @(t)(cos(j*t));
    norm_l = (abs(pswft(Tl,max(80,N+3))(l+3)) + abs(pswft(Tj,max(80,N+3))(j+1)));
    F(l+1)/= norm_l;
  endfor
  A = -alpha* B + gamma * A;
  %display(["A = "; num2str(A)]);
  % On modifie a present le systeme pour prendre en compte la parite
  K = floor(N/2);
  M1 = zeros(K,K);
  M2 = zeros(K,K);
  G1 = zeros(K,1);
  G2 = zeros(K,1);
  for l = [1:K]
    for k = [1:K]
      M1(l,k) = A(2*l-1,2*k-1);
      M2(l,k) = A(2*l,2*k);
    endfor
    G1(l) = F(2*l-1);
    G2(l) = F(2*l);
  endfor
endfunction
\end{lstlisting}

\hrule 

\begin{lstlisting}[language=Matlab]
 % Script qui permet de tester la méthode
clear; clc; close all;
p = 1;
f = @(x)(x);
alpha = 1;
gamma = 1;
N = 20;
[M1,M2,G1,G2] = syslin(alpha,gamma,f,N);
%display(["M1 = "; num2str(M1)])
%display(["M2 = "; num2str(M2)])
%display(["G1 = "; num2str(G1)])
%display(["G2 = "; num2str(G2)])

U1 = gauss(M1,G1);
U2 = gauss(M2,G2);

%display(["U1 = "; num2str(U1)])
%display(["U2 = "; num2str(U2)])

V1 = M1\G1;
V2 = M2\G2;

%display(["V1 = "; num2str(V1)])
%display(["V2 = "; num2str(V2)])

% On trouve que les deux méthodes de resolution donnent le meme resultat.

U = [V1;V2];
%display(["U = "; num2str(U)])

% On réordonne les éléments de U
U = reorder(U);
%display(["U = "; num2str(U)])

J = 250;
x = linspace(-1,1,J);
T = tchebychev(N+2,x);
figure
hold on
for k = [1:5]
  plot(x,T(k,:),"Displayname",num2str(k-1),"LineWidth",1.3)
endfor
title("Polynomes de Tchebychev")
xlabel("x")
ylabel("T_k(x)")
legend
hold off

Phi = zeros(N,J);
figure
hold on
for k = [1:N]
  j = mod(k-1,2);
  Phi(k,:) = T(k+2,:) - T(j+1,:);
  Tk = @(t)(cos((k+1)*t));
  Tj = @(t)(cos(j*t));
  norm_k = (abs(pswft(Tk,max(80,N+3))(k+3)) + abs(pswft(Tj,max(80,N+3))(j+1)));
  Phi(k,:) /= norm_k;
  if k < 6
    plot(x,Phi(k,:),"Displayname",num2str(k-1),"LineWidth",1.3)
  endif
endfor
xlabel("x")
ylabel("phi_k(x)")
title("Fonctions phi")
legend
hold off

[M1,M2,G1,G2] = syslin(alpha,gamma,f,N);
U1 = gauss(M1,G1);
U2 = gauss(M2,G2);
V1 = M1\G1;
V2 = M2\G2;
U = [V1;V2];
U = reorder(U);
sol = transpose(U) * Phi;


%sol_ex = @(x)(f(x)/gamma *(1 - cosh(sqrt(gamma) * x).* 1/cosh(sqrt(gamma))));
u1 = @(x)(2*exp(-x -1) - exp(x + 1) + x);
u2 = @(x)(1/2 * (exp(x + 1) - exp(-x-1)));
kappa = - u1(1)/u2(1) ;
sol_ex = @(x)(u1(x) + kappa * u2(x));

u1_exp = sedoci(x,@(xt)(alpha),@(t)(0),@(t)(0),@(t)(gamma),f,[0;0]);
u2_exp = sedoci(x,@(xt)(alpha),@(t)(0),@(t)(0),@(t)(gamma),f,[0;1]);
kappa = - u1_exp(J)/u2_exp(J);

sol_exp = u1_exp + kappa* u2_exp;

figure
hold on
plot(x,sol,"Displayname","Solution","LineWidth",1.5)
plot(x,sol_exp,"Displayname","Solution Tir","Color","black","LineWidth",1.5)
plot(x,sol_ex(x),"Displayname","Solution exacte","Color","red","LineWidth",1.5);
xlabel("x")
ylabel("u(x)")
title("Résultats pour -u'' + u = x ; u(-1) = u(1) = 0")
legend
hold off
\end{lstlisting}

\hrule 

\begin{lstlisting}[language=Matlab]
 %%% Script pour l'exercice 5 - transformee de Fourier rapide %%%%%%%%%%%%%%%%%%%

clear; clc; close all;

%%% Trace de fcos ==============================================================
p=1;
fcos = @(t) ((t.^p).*((2*pi.-t)).^p);
figure();
fplot(fcos, [0 , 2*pi], "DisplayName", "fcos");
xlabel("tet");
ylabel("f(cos(tet))");
title(["Trace de fcos pour p=",num2str(p)]);


%%% Trace des coefficients  ====================================================
J = 20;  N=85;
res=zeros(1,J);
K = 1:J;
coefs_reels = -2*pi./(K.^2);
coefs_reels = [2*pi**3/3,coefs_reels];
ps = pswft(fcos,N);
display(num2str(size(ps)));
%resint=[];
for i=0:J
  res(i+1)=ps(i+1);
endfor

figure();
hold on;
plot(0:J,res,"DisplayName","Coefs calculés","LineWidth",1.3);
plot(0:J,coefs_reels,"DisplayName","Coefs réels","LineWidth",1.3);
xlabel("n");
ylabel("Valeur");
title("Calcul des produits scalaires dans le cas p=1");
legend;
hold off;


%%% Convergence de la FFT  =====================================================
p=1; n=5;
fcos = @(t) ((t.^p).*((2*pi.-t)).^p);
N = [20:5:100];
coefs = zeros(1,length(N));
errs = zeros(1,length(N));
for i=[1:length(N)]
  coefs(i) = pswft(fcos,N(i))(n+1);
  errs(i) = abs(-2*pi/(n^2)-coefs(i))/(2*pi/(n^2));
endfor

upto=length(N)-3;
P=polyfit(N(1:upto), log(errs)(1:upto),1);
disp(["coefficient directeur par interpolation : ",num2str(P(1))]);

figure;
hold on;
plot(N,[log(errs); P(2).+P(1).*N],"LineWidth",1.3);
legend(["log(err)"; "droite d'interpolation"]);
title(["Erreur relative pour (p,n)=(", num2str(p),",", num2str(n),")"]);
xlabel("N");
ylabel("log(Erreur)");
hold off;


%%% Trace de fcos ==============================================================
p=4;
fcos = @(t) ((t.^p).*((2*pi.-t)).^p);
figure();
hold on;
fplot(fcos, [0 , 2*pi], "DisplayName", "fcos");
xlabel("tet");
ylabel("f(cos(tet))");
title(["Trace de fcos pour p=",num2str(p)]);
hold off;

%{
%%% Convergence de la FFT  =====================================================
p=4; n=0;
fcos = @(t) ((t.^p).*((2*pi.-t)).^p);
N = [20:5:300];
coefs = zeros(1,length(N));
errs = zeros(1,length(N));

fun=@(x) fcos(x).*cos(n.*x);
valex=256*pi/(2*315); %pswft(fcos,85)(n+1); %a remplacer par la valeur exacte
for i=[1:length(N)]
  coefs(i) = pswft(fcos,N(i))(n+1);
  errs(i) = abs((valex-coefs(i))/(valex));
endfor

upto=length(N)-3;
P=polyfit(N(1:upto), log(errs)(1:upto),1);
disp(["coefficient directeur par interpolation : ",num2str(P(1))]);

figure;
plot(N,[log(errs); P(2).+P(1).*N]);
legend(["log(err)"; "droite d'interpolation"]);
title(["Erreur relative pour (p,n)=(", num2str(p),",", num2str(n),")"]);
xlabel("N");
ylabel("log(Erreur)");
%}

%%% Temps de calcul compare a une methode d'integration naive ==================
p=4; N=255;
Iint=zeros(1,N);
Ifft=zeros(1,N);

ts=cputime;
fcos = @(tet) (tet.^p).*((2*pi.-tet).^p);
Ifft=pswft(fcos,N);
te=cputime;
tft=te-ts;

ts=cputime;
for n=0:N
  fun=@(x) fcos(x).*cos(n.*x);
  Iint(n+1)=integral(fun,0,2*pi)/2;
endfor
te=cputime;
tint=te-ts;

disp(["temps total de calcul par transform�e de fourier : ",num2str(tft)," s"]);
disp(["temps total de calcul par integration numerique : ",num2str(tint)," s"]);

\end{lstlisting}




\end{document}
